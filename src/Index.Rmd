---
title: 'Generalized linear model assignment: Group 8'
author: 
  - Jack Heller (r0862809)
  - Aleksandra Zdravkovic (r0869484)
  - Viktoria Kirichenko (r0877202)
  - Medha Hegde (r0872802)
  - Baris Aksoy (r0869901)
  - Ra√Øsa Carmen (s0204278)
date: "31-12-2021"
fontsize: 12pt
header-includes: \usepackage{booktabs}
output: 
  bookdown::pdf_document2:
    keep_tex:  true
    toc: false
    fig_caption: yes
    extra_dependencies: ["float"]
    includes:
      in_header: "preamble.tex"
  pdf_document:
    extra_dependencies: ["flafter"]
  bookdown::html_document2:
    fig_caption: yes
---

```{r setup, include=FALSE}
#notes on fonts, font size, line spacing https://bookdown.org/yihui/rmarkdown-cookbook/latex-variables.html
knitr::opts_chunk$set(echo = FALSE)
library(here)
library(tidyverse)
library(cowplot)
library(kableExtra)
library(multcomp)
library(DHARMa) # to simulate residuals
#read the data
library(vcd) #for the rootogram
data <- read.delim(file = sprintf("%s/data/homicidevictim.txt",here())) %>% 
  mutate(race = as.factor(race))
  
```

# Introduction
This report investigated the link between a persons' race and the number of homicide victims a person knows. `r nrow(data)` people were asked how many homicide victims they know. The raw data is analysed in section \@ref(EDA) after which several statistical models are explored in section \@ref(MET). Lastly, section \@ref(CON) concludes the report.

# Data exploration{#EDA}
In total, `r nrow(data)` respondents were asked how many homicide victims they knew. Figure \@ref(fig:Response) shows the absolute and relative number of respondents for each race that knew 0, 1, 2, 3, 4, 5, or 6 homicide victims. The same summary data is displayed in Table \@ref(tab:Responsetab). It`r` is clear that there are a lot more white participants in the study (`r sum(data$race =="white")` (`r round(sum(data$race =="white")/nrow(data)*100,digits=2)`%) white versus `r sum(data$race =="black")` (`r round(sum(data$race =="black")/nrow(data)*100,digits=2)`%) black people were questioned) and the relative frequencies show that black people know more homicide victims on average ( `r mean(data[data$race=="white","resp"])` known homicide victims per person on average for white and `r mean(data[data$race=="black","resp"])` for black participants).

```{r Response, include=TRUE, message = FALSE, fig.cap = "Absolute (A) and relative (B) number of respondents in each race and response group (number of homicide victims the respondent knows). The mean is indicated with a vertical line.", fig.height = 5, fig.width = 10}
summ <- data %>% group_by(race) %>%
  summarize(mean = mean(resp),
            median = median(resp),
            variance = var(resp))
p1 <- data %>% group_by(resp, race) %>%
  summarize(n = n()) %>% 
  complete(resp, race, fill = list(n = 0)) %>%
  distinct() %>%
  ungroup() %>%
  ggplot() + 
  geom_bar(aes(x = resp, fill = race, y = n), 
                 stat = "identity", position = "dodge", alpha = 0.5) +
  geom_text(aes(x = resp - 0.2*(race == "black") + 0.2*(race == "white"), 
                y = (n + 10), 
                label = n)) +
  geom_vline(data = summ,aes(color = race, xintercept = mean)) +
  ylab("Number of respondents in each race") + 
  xlab("Number of known homicide victims") +
  theme_bw() + theme(legend.position = c(0.8, 0.8))
p2 <- data %>% group_by(resp, race) %>%
  summarize(n = n()) %>% ungroup() %>% group_by(race) %>%
  mutate(perc = n/sum(n)) %>%
  complete(resp, race, fill = list(n = 0, perc = 0)) %>%
  distinct() %>%
  ungroup() %>%
  ggplot() + 
  geom_bar(aes(x = resp, fill = race, y = perc), 
                 stat = "identity", position = "dodge", alpha = 0.5) +
  geom_text(aes(x = resp - 0.2*(race == "black") + 0.2*(race == "white"), 
                y = (perc + 0.02), 
                label = sprintf("%d%%",round(perc*100)))) +
  geom_vline(data = summ,aes(color = race, xintercept = mean)) + 
  theme_bw() + theme(legend.position = c(0.8, 0.8)) + 
  scale_y_continuous(labels = scales::percent) +
  ylab("Percentage of respondents in each race") + 
  xlab("Number of known homicide victims")
plot_grid(p1, p2, labels = c('A', 'B'), label_size = 12)
```

```{r Responsetab, include=TRUE, message = FALSE}
data %>% group_by(race, resp) %>%
  summarize(n = n()) %>% 
  ungroup() %>% 
  group_by(race) %>%
  mutate(perc = n/sum(n)) %>%
  complete(race, resp, fill = list(n = 0, perc = 0)) %>%
  distinct() %>%
  arrange(race, resp) %>%
  mutate(perc = sprintf("%.2f%%",round(100*perc, 2))) %>%
  ungroup() %>%
  kable(booktabs = TRUE, 
        caption = "Summary data.", 
        col.names = str_wrap(c('Race', 'Response', 'Number of respondents',
                              'Percentage of respondents withing each race'),
                             width = 20 )) %>% 
  kable_styling()

```

# Methodology & results {#MET}

## Poisson model
Since the number of homocide victims a person knows is count data, a Poisson model is first applied to the data (Table \@ref(tab:poissonmodel)). 

```{r poissonmodel,include=TRUE, message = FALSE}
mod.poisson <- glm(resp ~ race, family = poisson, data = data)
sum.mod.poisson <- summary(mod.poisson)

sum.mod.poisson$coefficients %>%
  kable(booktabs = T, 
        caption = "Poisson model.") %>% kable_styling()
#the following function calculates risk ratios with confidence interval. Source: https://rpubs.com/kaz_yos/poisson
glm.RR <- function(GLM.RESULT, digits = 2) {

    if (GLM.RESULT$family$family == "binomial") {
        LABEL <- "OR"
    } else if (GLM.RESULT$family$family == "poisson") {
        LABEL <- "RR"
    } else {
        stop("Not logistic or Poisson model")
    }

    COEF      <- stats::coef(GLM.RESULT)
    CONFINT   <- stats::confint(GLM.RESULT)
    TABLE     <- cbind(coef=COEF, CONFINT)
    TABLE.EXP <- round(exp(TABLE), digits)

    colnames(TABLE.EXP)[1] <- LABEL

    TABLE.EXP
}
poissonrr <- glm.RR(mod.poisson)
```

The model shows that white respondents know less homicide victims, on average, than black respondents. Indeed, the risk ratio in table \@ref(tab:poissonrr) shows that the number of known homicide victims for white respondents is `r poissonrr[2,1]` (between `r poissonrr[2,2]` and `r poissonrr[2,3]` with a confidence level of 95%) times the average number of homicide victims that  black respondents know on average.
Since the poisson regression models the log mean of the poisson regression, the mean number of homicide victims for black people is estimated to be exp(`r sum.mod.poisson$coefficients[1,1]`) = `r exp(sum.mod.poisson$coefficients[1,1])` and exp(`r sum.mod.poisson$coefficients[1,1]` + `r sum.mod.poisson$coefficients[2,1]`) = `r exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1])` for white individuals. Those averages are exactly equal to the observed values (section \@(ref:EDA)). The ratio of the mean responses is `r exp(sum.mod.poisson$coefficients[1,1] )/exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1])` (black/white) and `r exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1])/exp(sum.mod.poisson$coefficients[1,1])` (white/black). This means that, on average, a black person knows `r round(exp(sum.mod.poisson$coefficients[1,1] )/exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1]), digits=2)` times more homicide victims than a white person notice that `r exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1])/exp(sum.mod.poisson$coefficients[1,1])` = 1/`r round(exp(sum.mod.poisson$coefficients[1,1] )/exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1]), digits=2)`.

```{r poissonrr,include=TRUE, message = FALSE}
kable(poissonrr, 
        caption = "Poisson risk ratios.") %>% kable_styling()
```

Figure \@ref(fig:poissonpredict) compares the true data with the predicted probabilities. Although the model is very accurate with respect to the mean, it is clear that the variance is larger in reality than in the Poisson model. Furthermore, there may be some zero-inflation, especially for the black population.

```{r poissonpredict, include=TRUE, message = FALSE, fig.cap = "True and predited probabilities of respondents knowing a certain number of homicide victims, for each race.", fig.width = 10, fig.height = 6}
ratewhite <- exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1])
rateblack <- exp(sum.mod.poisson$coefficients[1,1])

data %>% group_by(resp, race) %>%
  summarize(n = n()) %>% ungroup() %>% group_by(race) %>%
  mutate(perc = n/sum(n)) %>%
  complete(resp, race, fill = list(n = 0, perc = 0)) %>%
  distinct() %>%
  ungroup() %>%
  mutate(rate = ifelse(race=="black", rateblack,ratewhite),
         prediction = rate^(resp)/factorial(resp)*exp(-1*rate)) %>%
  pivot_longer(cols = c(perc,prediction), names_to = "type", 
               values_to = "value") %>%
  ggplot() + 
  geom_bar(aes(x = resp, y = value,fill = type), stat = "identity", 
           position = "dodge") +
  geom_text(aes(x = resp - 0.2*(type == "perc") + 0.2*(type == "prediction"), 
                y = (value + 0.02), 
                label = sprintf("%d%%", round(value * 100)))) +
  theme_bw() + theme(legend.position = c(0.8, 0.8)) + 
  scale_y_continuous(labels = scales::percent) +
  ylab("Percentage of predicted or true respondents in each race") + 
  xlab("Number of known homicide victims") +
  facet_grid(cols = vars(race)) +
  scale_fill_discrete(name = "", labels = c('True probability','Predicted probability'))
```

Indeed, one important assumption in a Poisson model, is that the mean is equal to the variance. The variance in the data is `r var(data$resp)` (`r unname(unlist(summ[1,4]))` for black and `r unname(unlist(summ[2,4]))` for white respondents). Figure \@ref(fig:GOF1) below shows there is overdispersion (the real variance in red is larger than the simulated variance). 

```{r include=TRUE, message = FALSE}
sim.mod.poisson <- simulateResiduals(mod.poisson, plot = T)
```
```{r include=TRUE, message = FALSE}
hist(sim.mod.poisson)
```
```{r GOF1, include=TRUE, message = FALSE, fig.cap = "Test of uniformity (A) and dispersion (B)", fig.width = 10, fig.height = 6}
testUniformity(sim.mod.poisson)
testDispersion(sim.mod.poisson)
```

## Negative-binomial model
This model uses the Pascal distribution which counts the number of failures before the y$^{th}$ success. If $x \sim NB(y,\pi)$ with $\pi$ the probability of success;

\begin{equation} 
\begin{aligned}
E(x) = \mu = \frac{y\pi}{1-\pi} \\
  Var(x) = \sigma^2 = \frac{y\pi}{(1-\pi)^2} = \mu+\frac{1}{\theta}\mu^2
  (\#eq:binom)
\end{aligned}
\end{equation} 
 This means that the negative binomial model assumes a quadratic relationship between the mean and the variance. 
 
```{r include=FALSE, message = FALSE}
library(MASS)
mod.nb <- glm.nb(resp ~ race, data = data)
sumnb <- summary(mod.nb)
sumnb
munbblack <- exp(-unlist(mod.nb$coefficients[1]))
thetanb <- mod.nb$theta
munbwhite <- exp(-unlist(mod.nb$coefficients[1]) + unlist(mod.nb$coefficients[2]))
```

The variance for each of the races can be obtained from the equation $\sigma^2 = \mu+\frac{1}{\theta}\mu^2$ where $\mu = e^{x' \hat{\beta} }$. For black people, $\mu =$ `r unname(munbblack)` and the variance is `r unname(munbblack + 1/thetanb*munbblack^2)`. For white people, $\mu =$ `r unname(munbwhite)` and the variance is `r unname(munbwhite + 1/thetanb*munbwhite^2)`.

## Quasi-likelihood model
In quasi-likelihood models, the mean and variance function are specified separately. This thus lifts the poisson assumption that mean and variance are equal. In general, if the mean structure is specified as $\lambda = \mu(x,\beta) = e^{x'\beta}$, then the variance is $var(y_i) = \phi\lambda$ where $\hat{\beta}$ and $\phi$ are estimated from the Pearson statistic. This model thus assumes a linear relationship between the mean and variance.
```{r include=TRUE, message = FALSE}
mod.quasip <- glm(resp ~ race, family = quasipoisson, data = data)
sum.quasip <- summary(mod.quasip)
sum.quasip
```

The regression results show that the dispersion parameter $\psi$ is estimated to be `r sum.quasip$dispersion` which is much larger than one (Poisson model assumes it to be one).

## Sandwich-estimator
```{r include=TRUE, message = FALSE}
library(sandwich)
library(lmtest)
coeftest(mod.poisson, vcov = sandwich)
#mod.poisson.empty <- glm(resp ~ 1, family = poisson, data = data)
#waldtest(mod.poisson, mod.poisson.empty,  vcoc = sandwich, test = "Chisq") # only works without the "sandwich"
```


## Zero-inflated models
Lastly, a zero-inflated Poisson model and negative binomial was tested because the raw data showed that there were many people the knew no homicide victims. 

```{r include=TRUE, message = FALSE}
library(pscl)
mod.zip <- zeroinfl(resp ~ race | race,  dist = 'poisson', data = data)
mnull <- update(mod.zip, . ~ 1)
pchisq(2 * (logLik(mod.zip) - logLik(mnull)), df = 3, lower.tail = FALSE)
summary(mod.zip)
E2 <- resid(mod.zip, type = "pearson")
N  <- nrow(data)
p  <- length(coef(mod.zip))  
disp.zip <- sum(E2^2) / (N - p)#1.09dispersion statistic gets really close to one!
aic.zip <- AIC(mod.zip)
```

The zero-inflated poisson model shows that white people are significantly more likely to know no homicide victims and the poisson regression coefficient for white people is still negative  and highly significant. 

```{r include=TRUE, message = FALSE}
library(pscl)
mod.zinb <- zeroinfl(resp ~ race | race,  dist = 'negbin', data = data)
mnull <- update(mod.zinb, . ~ 1)
pchisq(2 * (logLik(mod.zinb) - logLik(mnull)), df = 3, lower.tail = FALSE)
summary(mod.zinb)
E2 <- resid(mod.zinb, type = "pearson")
N  <- nrow(data)
p  <- length(coef(mod.zinb))  
disp.zinb <- sum(E2^2) / (N - p)#1.06dispersion statistic gets really close to one!
aic.zinb <- AIC(mod.zinb)
```

# Conclusion {#CON}
